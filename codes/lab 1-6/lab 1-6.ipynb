{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fce631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce10f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f403c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset missing - downloading...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/mnist/temp/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/mnist/temp/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/mnist/temp/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n",
      "2.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/mnist/temp/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/mnist/temp/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/mnist/temp/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/mnist/temp/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/mnist/temp/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/mnist/temp/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/mnist/temp/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/mnist/temp/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/mnist/temp/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/hjj/opt/anaconda3/envs/deeplearn_course/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459064158/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "from utils import check_mnist_dataset_exists\n",
    "data_path = check_mnist_dataset_exists()\n",
    "\n",
    "# download mnist dataset\n",
    "# 60,000 gray scale pictures as well as their label, each picture is 28 by 28 pixels\n",
    "train_data = torch.load(data_path + 'mnist/train_data.pt')\n",
    "train_label = torch.load(data_path + 'mnist/train_label.pt')\n",
    "test_data = torch.load(data_path + 'mnist/test_data.pt')\n",
    "test_label = torch.load(data_path + 'mnist/test_label.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bec899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class three_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,  output_size):\n",
    "        super(three_layer_net , self).__init__()\n",
    "        \n",
    "        # 三层全连接网络MLP\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1, bias=False)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2, bias=False)\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = self.layer1(x)\n",
    "        # 第一层使用ReLU作为激活函数\n",
    "        y_hat = torch.relu(y)\n",
    "        z = self.layer2(y_hat)\n",
    "        # 第二层使用ReLU作为激活函数\n",
    "        z_hat = torch.relu(z)\n",
    "        # 第三层直接输出\n",
    "        scores = self.layer3(z_hat)\n",
    "        # prob = torch.softmax(y, dim=1)\n",
    "        # 若这里用了softmax，则\n",
    "        # 1. criterion需要用NLLLoss\n",
    "        # 2. 需要对输出的分数求log，即log_scores = torch.log(scores)\n",
    "        # 3. 最终的损失loss = criterion(log_scores, label)\n",
    "        # 以上的步骤其实就是Cross-Entropy Loss的拆分，NLLLoss实际就是在做归一化\n",
    "        # 若使用LogSoftmax\n",
    "        # prob = torch.logsoftmax(y, dim=1)\n",
    "        # 则不需要求log\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec1db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_layer_net(\n",
      "  (layer1): Linear(in_features=784, out_features=50, bias=False)\n",
      "  (layer2): Linear(in_features=50, out_features=50, bias=False)\n",
      "  (layer3): Linear(in_features=50, out_features=10, bias=False)\n",
      ")\n",
      "There are 42200 (0.04 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "net = three_layer_net(784, 50, 50, 10)\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfa132cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d435ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.NLLLoss()\n",
    "# batch size = 200\n",
    "bs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceef4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_error = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # test size = 10000\n",
    "    for i in range(0, 10000, bs):\n",
    "\n",
    "        # extract the minibatch\n",
    "        minibatch_data = test_data[i: i + bs]\n",
    "        minibatch_label = test_label[i: i + bs]\n",
    "\n",
    "        # reshape the minibatch, 784 = 28 x 28\n",
    "        # 200 x 784\n",
    "        inputs = minibatch_data.view(bs, 784)\n",
    "\n",
    "        # feed it to the network\n",
    "        scores = net(inputs) \n",
    "\n",
    "        # compute the error made on this batch\n",
    "        error = utils.get_error(scores, minibatch_label)\n",
    "\n",
    "        # add it to the running error\n",
    "        running_error += error.item()\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "    total_error = running_error / num_batches\n",
    "    print('test error  = ', total_error * 100, ' percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c7554e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch =  0  time =  0.5236721038818359  loss =  1.3819054213166237  error =  36.825000027815506  percent lr =  0.05\n",
      "test error  =  14.660000085830688  percent\n",
      " \n",
      "epoch =  10  time =  5.694929122924805  loss =  0.17998844516774018  error =  5.126667161782582  percent lr =  0.05\n",
      "test error  =  5.349999785423279  percent\n",
      " \n",
      "epoch =  20  time =  10.660156011581421  loss =  0.10820389812812209  error =  3.06499973932902  percent lr =  0.03333333333333333\n",
      "test error  =  3.700000047683716  percent\n",
      " \n",
      "epoch =  30  time =  15.705405235290527  loss =  0.0824780416302383  error =  2.32499893506368  percent lr =  0.022222222222222223\n",
      "test error  =  3.18999981880188  percent\n",
      " \n",
      "epoch =  40  time =  20.70580005645752  loss =  0.07029626129815976  error =  1.9466654459635417  percent lr =  0.014814814814814815\n",
      "test error  =  3.039999485015869  percent\n",
      " \n",
      "epoch =  50  time =  25.767770051956177  loss =  0.06325585604955752  error =  1.714998682339986  percent lr =  0.009876543209876543\n",
      "test error  =  2.889999508857727  percent\n",
      " \n",
      "epoch =  60  time =  30.847910165786743  loss =  0.059318346610913675  error =  1.6049987872441611  percent lr =  0.006584362139917695\n",
      "test error  =  2.8999996185302734  percent\n",
      " \n",
      "epoch =  70  time =  35.97524809837341  loss =  0.05679505726943413  error =  1.52999875942866  percent lr =  0.0043895747599451305\n",
      "test error  =  2.8599995374679565  percent\n",
      " \n",
      "epoch =  80  time =  41.13825511932373  loss =  0.05518298687723776  error =  1.4816654523213706  percent lr =  0.0029263831732967535\n",
      "test error  =  2.779999613761902  percent\n",
      " \n",
      "epoch =  90  time =  46.52946925163269  loss =  0.0541352233539025  error =  1.4583322008450825  percent lr =  0.001950922115531169\n",
      "test error  =  2.799999713897705  percent\n",
      " \n",
      "epoch =  100  time =  51.739004135131836  loss =  0.053453979678452015  error =  1.4333320458730063  percent lr =  0.001300614743687446\n",
      "test error  =  2.789999485015869  percent\n",
      " \n",
      "epoch =  110  time =  57.318917989730835  loss =  0.05299141206468145  error =  1.413332144419352  percent lr =  0.0008670764957916306\n",
      "test error  =  2.8099995851516724  percent\n",
      " \n",
      "epoch =  120  time =  62.9280219078064  loss =  0.05269407430353264  error =  1.4149987300237021  percent lr =  0.0005780509971944203\n",
      "test error  =  2.7599995136260986  percent\n",
      " \n",
      "epoch =  130  time =  68.42490100860596  loss =  0.05248700404850145  error =  1.4116654594739277  percent lr =  0.0003853673314629469\n",
      "test error  =  2.7599995136260986  percent\n",
      " \n",
      "epoch =  140  time =  73.75059103965759  loss =  0.052353302389383315  error =  1.4083321690559387  percent lr =  0.00025691155430863124\n",
      "test error  =  2.7599995136260986  percent\n",
      " \n",
      "epoch =  150  time =  79.07867407798767  loss =  0.05226605800911784  error =  1.4033320148785908  percent lr =  0.0001712743695390875\n",
      "test error  =  2.7699995040893555  percent\n",
      " \n",
      "epoch =  160  time =  84.24923801422119  loss =  0.05220504401251674  error =  1.4049987991650899  percent lr =  0.00011418291302605833\n",
      "test error  =  2.7699995040893555  percent\n",
      " \n",
      "epoch =  170  time =  89.68895888328552  loss =  0.05216534295429786  error =  1.404998819033305  percent lr =  7.612194201737223e-05\n",
      "test error  =  2.7699995040893555  percent\n",
      " \n",
      "epoch =  180  time =  95.66823291778564  loss =  0.052138819561029476  error =  1.3999988039334614  percent lr =  5.074796134491482e-05\n",
      "test error  =  2.7699995040893555  percent\n",
      " \n",
      "epoch =  190  time =  101.20402908325195  loss =  0.052121049122263986  error =  1.401665469010671  percent lr =  3.3831974229943214e-05\n",
      "test error  =  2.7699995040893555  percent\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lr = 0.05 # initial learning rate\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    # learning rate strategy: divide the learning rate by 1.5 every 10 epochs\n",
    "    if epoch % 10 == 0 and epoch > 10:\n",
    "        lr = lr / 1.5\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.\n",
    "    optimizer = torch.optim.SGD(net.parameters() , lr=lr)\n",
    "        \n",
    "    running_loss = 0\n",
    "    running_error = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # 先随机排序\n",
    "    # train size = 60000\n",
    "    shuffled_indices = torch.randperm(60000)\n",
    "\n",
    "    # train size = 60000\n",
    "    for count in range(0, 60000, bs):\n",
    "        \n",
    "        # forward and backward pass\n",
    "        # set dL/dU, dL/dV, dL/dW to be filled with zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 随机抽取200条数据, batch size = 200\n",
    "        indices = shuffled_indices[count: count + bs]\n",
    "        minibatch_data = train_data[indices]\n",
    "        minibatch_label = train_label[indices]\n",
    "\n",
    "        # reshape the minibatch, batch size = 200, 784 = 28 x 28\n",
    "        # 200 x 784\n",
    "        inputs = minibatch_data.view(bs, 784)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net\n",
    "        scores = net(inputs) \n",
    "        # log_scores = torch.log(scores)\n",
    "\n",
    "        # compute the average of the losses of the data points in the minibatch\n",
    "        # 一个batch的平均损失\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        # loss = criterion(log_scores, minibatch_label)\n",
    "        \n",
    "        # backward pass to compute dL/dU, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute some stats\n",
    "        # 获得当前的loss\n",
    "        running_loss += loss.detach().item()\n",
    "               \n",
    "        error = utils.get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches += 1\n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
    "    # 总Loss = 每个Batch的Loss累加 / Batch数量累加 = 所有Batch的Loss / Batch数\n",
    "    # 若Batch Size = 1，则Batch数 = 数据集大小\n",
    "    # 若Batch Size = 数据集大小，则Batch数 = 1\n",
    "    total_loss = running_loss / num_batches\n",
    "    # 总Error = 每个Batch的Error累加 / Error数量累加 = 所有Batch的Error / Batch数\n",
    "    # 若Batch Size = ，则Batch数 = 数据集大小\n",
    "    # 若Batch Size = 数据集大小，则Batch数 = 1\n",
    "    total_error = running_error / num_batches\n",
    "    # 训练一个batch的时间\n",
    "    elapsed_time = time.time() - start\n",
    "    \n",
    "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
    "    if epoch % 10 == 0 : \n",
    "        print(' ')\n",
    "        print('epoch = ', epoch, ' time = ', elapsed_time, ' loss = ', total_loss, ' error = ', total_error * 100, ' percent lr = ', lr)\n",
    "        eval_on_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28252180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a picture at random\n",
    "idx = randint(0, 10000-1)\n",
    "im = test_data[idx]\n",
    "\n",
    "# diplay the picture\n",
    "utils.show(im)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "# im.view(1, 784)而不是im.view(784)是因为net是根据有batch size存在而设计的\n",
    "# 例如batch size = 200，即im.view(200, 784)，则input是[[data_1], [data_2], ..., [data_200]]\n",
    "# 而im.view(784)的input是[data_1]，少了一个维度\n",
    "# 1代表了batch size = 1，就是只有一张图\n",
    "scores = net(im.view(1, 784)) # one 1 x 784 image, 784 = 28 x 28\n",
    "# dim=1是因为这里的输出是\n",
    "# [[-7.2764, 8.4730, 2.6842, 1.6302, -3.8437, -1.9697, -0.5854, -0.0792, 2.0861, -0.5462]]\n",
    "# 需要求里面的维度的softmax\n",
    "probs = torch.softmax(scores, dim=1)\n",
    "utils.show_prob_mnist(probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
